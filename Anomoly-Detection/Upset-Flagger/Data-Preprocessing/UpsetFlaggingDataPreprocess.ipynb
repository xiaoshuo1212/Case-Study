{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the libraries to be imported are defined here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# from .supporting_functions import process_stage_nosrom\n",
    "# from upset_flagger.upset.upset_scoring import score_upset_subevent\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "upset_flagger_path = 'C:/Users/XiaoShuoCui/Documents/GitHub/suncor/upset_flagging/'\n",
    "sys.path.append(upset_flagger_path) ## to use the module(upset flagging folder) defined under this path later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upset_flagger.upset.upset_scoring import score_upset_subevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = json.load(open(upset_flagger_path + 'upset_config.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_se = 'HPW Tank Temperature'  # Target upset subevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asset_id': 'a6e127fe-e718-4f78-a483-67f75b69d43d',\n",
       " 'upset_subevent': 'HPW Tank Temperature',\n",
       " 'rule_type': 'special hpw tank temperature',\n",
       " 'lookback': 11,\n",
       " 'resolution': 5,\n",
       " 'robustness': 1,\n",
       " 'tag_names': ['82TI867'],\n",
       " 'window_size': 36,\n",
       " 'ui_subevent_label': 'Tank Temperature Drop',\n",
       " 'ui_threshold_label': ' -2 F/hr',\n",
       " 'options': [],\n",
       " 'persistence_type': 'sustained increase',\n",
       " 'persistence_robustness': 3,\n",
       " 'persistence_threshold': 0.01,\n",
       " 'threshold_change': -1.1111,\n",
       " 'threshold_temp': 82.2222}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the appropriate upset flagging config\n",
    "for ut in config_data['upset_types']:\n",
    "    for se in ut['flags']:\n",
    "        if se['upset_subevent'] == target_se:\n",
    "            se_config = copy.copy(se)\n",
    "            break\n",
    "upset_flag_config = se_config\n",
    "upset_flag_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter preparation\n",
    "''' New configuration which leverages FPA functionality\n",
    "\n",
    "    hw1: window sizes for computing differences (x(t) - x(t-w)) where w is the window size\n",
    "    hw2: window sizes for getting previous values to include in the current prediction record. This\n",
    "        is essentialy a type of flattening to incorporate prior values of features. A window size \n",
    "        of w means you would get x(t-w).\n",
    "    dead period: how much in advance to try to force the predictor to anticipate the actual upset\n",
    "    window size: size of the prediction window (prior to dead period) in which to generate positive \n",
    "        samples. \n",
    "    special functions: list of special functions, which need to be defined in base/base_upset.py\n",
    "        these functions will be expanded and their tags added to the list of tags (tags_extended)\n",
    "        the special function name should not appear in the tags list\n",
    "    tags: list of tags on which feature generation will occur, should include categorical tags and weather tags\n",
    "    categorical_tags: list of tags which should be handled as categorical\n",
    "    weather_tags: tags which come from weather data (do we need this distinction?)\n",
    "\n",
    "'''\n",
    "\n",
    "# original\n",
    "hw1 = [\"{}min\".format(k) for k in range(55, 0, -5)]\n",
    "hw2 = [\"{}min\".format(k) for k in range(25, 0, -5)]\n",
    "\n",
    "modeling_params = {\n",
    "    'timestamp_column': 'timestamp_utc',\n",
    "    'upset_flag_config': upset_flag_config,\n",
    "    'special_functions': ['87FI510_60F+4FI_58_60F'],\n",
    "    'tags': ['BOILER12_STAT', '25TI982', '35FY510A', '39TI317', '31FY2', '31PIC19', '35FY110A', \n",
    "             'BOILER4_STAT', 'BOILER13_STAT', '39TI402', 'BOILER2_STAT', 'P3_3FC311_TOT', \n",
    "             '31FY3', '35FY210A', '31PIC18', '31-FIT-1', 'BOILER3_STAT', '31PI1A', '82LC860', \n",
    "             '39PIC573', '82TI867', 'P3_3E1_HPW_TEMP', 'BOILER1_STAT', '25FI29', \n",
    "             'BOILER14_STAT', '39FI104', 'P39_GTG5_STATUS', '4MKC10FE902', 'P16_PEW_TEMP_TOT', \n",
    "             '3MKC10FE902', '31FIC172', 'temperature', '35FY410A', '57FC1261', '39FI204', \n",
    "             '57TI1502', 'PEW1_HPW_FLOW', 'BOILER15_STAT'\n",
    "            ], # list of regular tags to include\n",
    "    'categorical_tags': ['BOILER14_STAT', 'BOILER12_STAT', 'BOILER2_STAT', 'BOILER3_STAT', \n",
    "                         'BOILER13_STAT', 'P39_GTG5_STATUS', 'BOILER1_STAT', 'BOILER4_STAT', \n",
    "                         'BOILER15_STAT'],\n",
    "    'conversions': { \n",
    "        'null_to_zero_tags': [],\n",
    "    },\n",
    "    'fpa_params': {\n",
    "        'failure_detection_window': '30min', # default is 5 min\n",
    "        'dead_period': '0min',\n",
    "        'stages': [\n",
    "                   { 'window_sizes': hw1,\n",
    "                     'simple_functions': [None]*len(hw1),\n",
    "                     'advanced_functions': [['delta_diff']]*len(hw1),\n",
    "                     'tags': 'numeric',\n",
    "                   },\n",
    "                   { 'window_sizes': hw2,\n",
    "                     'simple_functions': [None]*len(hw2),\n",
    "                     'advanced_functions': [['past_value']]*len(hw2),\n",
    "                     'tags': 'all'\n",
    "                   }\n",
    "                  ]\n",
    "    },\n",
    "    'debug': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeriesOneHotEncoder(TransformerMixin):\n",
    "    def __init__(self, onehot_name_format=\"{}_cat={}\"):\n",
    "        self.onehot_nf = onehot_name_format\n",
    "        self.colnames = None\n",
    "        self.encoder = None\n",
    "\n",
    "    def _check_input(self, X):\n",
    "        # check for pandas series\n",
    "        if not isinstance(X, pd.Series):\n",
    "            raise ValueError(\"Data passed to one-hot encoding function is not a pandas.Series\")\n",
    "\n",
    "    def _transform(self, v, mask, index, colname):\n",
    "        v_enc = self.encoder.transform(v[mask].reshape(-1, 1))\n",
    "\n",
    "        v_enc_final = np.zeros(shape=(mask.shape[0], v_enc.shape[1]))\n",
    "        v_enc_final[mask] = v_enc\n",
    "\n",
    "        names = [self.onehot_nf.format(colname, af) for af in self.encoder.categories_[0]]\n",
    "        self.colnames = names\n",
    "        return pd.DataFrame(v_enc_final, index=index, columns=names)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **kwargs):\n",
    "\n",
    "        self._check_input(X)\n",
    "        v = X.values\n",
    "        colname = X.name\n",
    "\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        mask = ~np.isnan(v)\n",
    "        self.encoder.fit(v[mask].reshape(-1, 1))\n",
    "\n",
    "        return self._transform(v, mask, X.index, colname)\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "\n",
    "        self._check_input(X)\n",
    "        v = X.values\n",
    "        colname = X.name\n",
    "        mask = ~np.isnan(v)\n",
    "\n",
    "        return self._transform(v, mask, X.index, colname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialFunctions(object):\n",
    "    def __init__(self):\n",
    "        self.specials = {\n",
    "            '87FI510_60F+4FI_58_60F': {\n",
    "                'tags': ['87FI510_60F', '4FI_58_60F'],\n",
    "                'function': self.ips_flow,\n",
    "            },\n",
    "            '4VOL_13CHG - (300FC5 + 86FIC613)': {\n",
    "                'tags': ['4VOL_13CHG', '300FC5', '86FIC613'],\n",
    "                'function': self.vol_change,\n",
    "            },\n",
    "            '87FI122-4FI_70': {\n",
    "                'tags': ['87FI122', '4FI_70'],\n",
    "                'function': self.froth_flow,\n",
    "            },\n",
    "            '87_FI_SUM': {\n",
    "                'tags': ['87FI205', '87FI235', '87FI265', '87FI1205', '87FI305', '87FI335', '87FI365', '87FI1305'],\n",
    "                'function': self.feed_87,\n",
    "            },\n",
    "            'NHT1': {\n",
    "                'tags': ['7FIC3', 'PROD_PLAN_2W_UPG_NHT1_PROD', 'PROD_PLAN_2W_UPG_NHT1_PROD'],\n",
    "                'function': self.nht1,\n",
    "            },\n",
    "            'NHT2': {\n",
    "                'tags': ['P55_NHU_CHG_TOT', 'PROD_PLAN_2W_UPG_NHT2_PROD', 'PROD_PLAN_2W_UPG_NHT2_PROD'],\n",
    "                'function': self.nht2,\n",
    "            },\n",
    "            'NHT3': {\n",
    "                'tags': ['P64_NHU_CHG_TOT', 'PROD_PLAN_2W_UPG_NHT3_PROD', 'PROD_PLAN_2W_UPG_NHT3_PROD'],\n",
    "                'function': self.nht3,\n",
    "            },\n",
    "            'KHT': {\n",
    "                'tags': ['7FIC20', 'PROD_PLAN_2W_UPG_KHT_PROD', 'PROD_PLAN_2W_UPG_KHT_PROD'],\n",
    "                'function': self.kht,\n",
    "            },\n",
    "            'GOHT1': {\n",
    "                'tags': ['7FIC_39_40_41', 'PROD_PLAN_2W_UPG_GOTH1_PROD', 'PROD_PLAN_2W_UPG_GOTH1_PROD'],\n",
    "                'function': self.goht1,\n",
    "            },\n",
    "            'GOHT2': {\n",
    "                'tags': ['P55_GOHU_CHG_TOT', 'PROD_PLAN_2W_UPG_GOTH2_PROD', 'PROD_PLAN_2W_UPG_GOTH2_PROD'],\n",
    "                'function': self.goht2,\n",
    "            },\n",
    "            'DHT': {\n",
    "                'tags': ['P55_DHU_CHG_TOT', 'PROD_PLAN_2W_UPG_DTH1_PROD', 'PROD_PLAN_2W_UPG_DTH1_PROD'],\n",
    "                'function': self.dht,\n",
    "            },\n",
    "            'MILL_OILSAND_MASS_1HOUR_AVG': {\n",
    "                'tags': ['MILL_OILSAND_MASS'],\n",
    "                'function': self.mill_oilsand_mass_1hour_avg,\n",
    "            }\n",
    "        }\n",
    "        self.consts = {'bbl_per_m3': 6.29}\n",
    "        self.functions = {k: partial(v['function'], consts=self.consts) for k, v in self.specials.items()}\n",
    "        self.tag_map = {k: v['tags'] for k, v in self.specials.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def ips_flow(df_in, consts=None):\n",
    "        return df_in['87FI510_60F'] + df_in['4FI_58_60F']\n",
    "\n",
    "    @staticmethod\n",
    "    def vol_change(df_in, consts=None):\n",
    "        return df_in['4VOL_13CHG']/consts['bbl_per_m3'] - (df_in['300FC5'] + df_in['86FIC613'])\n",
    "\n",
    "    @staticmethod\n",
    "    def froth_flow(df_in, consts=None):\n",
    "        return df_in['87FI122']-df_in['4FI_70']\n",
    "\n",
    "    @staticmethod\n",
    "    def feed_87(df_in, consts=None):\n",
    "        return df_in['87FI205'] + df_in['87FI235'] + df_in['87FI265'] + df_in['87FI1205'] + df_in['87FI305'] + df_in['87FI335'] + df_in['87FI365'] + df_in['87FI1305']\n",
    "\n",
    "    @staticmethod\n",
    "    def nht1(df_in, consts=None):\n",
    "        return (df_in['7FIC3'] - df_in['PROD_PLAN_2W_UPG_NHT1_PROD']) / df_in['PROD_PLAN_2W_UPG_NHT1_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def nht2(df_in, consts=None):\n",
    "        return (df_in['P55_NHU_CHG_TOT'] - df_in['PROD_PLAN_2W_UPG_NHT2_PROD']) / df_in['PROD_PLAN_2W_UPG_NHT2_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def nht3(df_in, consts=None):\n",
    "        return (df_in['P64_NHU_CHG_TOT'] - df_in['PROD_PLAN_2W_UPG_NHT3_PROD']) / df_in['PROD_PLAN_2W_UPG_NHT3_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def kht(df_in, consts=None):\n",
    "        return (df_in['7FIC20'] - df_in['PROD_PLAN_2W_UPG_KHT_PROD']) / df_in['PROD_PLAN_2W_UPG_KHT_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def goht1(df_in, consts=None):\n",
    "        return (df_in['7FIC_39_40_41'] - df_in['PROD_PLAN_2W_UPG_GOTH1_PROD']) / df_in['PROD_PLAN_2W_UPG_GOTH1_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def goht2(df_in, consts=None):\n",
    "        return (df_in['P55_GOHU_CHG_TOT'] - df_in['PROD_PLAN_2W_UPG_GOTH2_PROD']) / df_in['PROD_PLAN_2W_UPG_GOTH2_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def dht(df_in, consts=None):\n",
    "        return (df_in['P55_DHU_CHG_TOT'] - df_in['PROD_PLAN_2W_UPG_DTH1_PROD']) / df_in['PROD_PLAN_2W_UPG_DTH1_PROD']\n",
    "\n",
    "    @staticmethod\n",
    "    def mill_oilsand_mass_1hour_avg(df_in, consts=None):\n",
    "        return (df_in['MILL_OILSAND_MASS'].rolling(12).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "Here we learnt the decorator @staticmethod:\n",
    "1. What: \n",
    "It is a decorator which is simply a normal function that is logically contained in the class for readability purposes. It can be called without instantiating the class first, which means we do not need to pass the class instance as the first argument via self, unlike other class functions.\n",
    "2. Access: \n",
    "Static methods can also be accessed via class instances or objects.\n",
    "3. VS Class mthd: \n",
    "Class methods are called via the Class containing it, rather than from an instance. Which is why a classmethod is defined as class_method(cls, param1, param2), which does not contain self, which means that it cannot be called using a class instance. Static methods can be called from both a class instance as well as from a Class.\n",
    "4. VS Instance mthd:\n",
    "Instance methods can only be called from a class instance, which is why any instance method is of the forminstance_method(self, param1, param2), where the self keyword signifies the class instance calling the method. @staticmethod can be called from both a class instance as well as from a Class.\n",
    "\n",
    "REF: https://www.askpython.com/python/staticmethod-in-python\n",
    "\n",
    "The partial function from functool:\n",
    "1. What:\n",
    "它是对原始函数的二次封装，是将现有函数的部分参数预先绑定为指定值，从而得到一个新的函数，该函数就称为偏函数。相比原函数，偏函数具有较少的可变参数，从而降低了函数调用的难度。\n",
    "2. Formula:\n",
    "偏函数名 = partial(func, * args, ** kwargs)\n",
    "其中，func 指的是要封装的原函数，*args 和 ** kwargs 分别用于接收位置实参和关键字实参。\n",
    "\n",
    "REF:http://c.biancheng.net/view/5674.html AND https://zhuanlan.zhihu.com/p/47124891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_base_data(df,\n",
    "                   mode='deploy',\n",
    "                   timestamp_column='timestamp_utc',\n",
    "                   upset_flag_config=None,\n",
    "                   special_functions=None,\n",
    "                   tags=None,\n",
    "                   categorical_tags=None,\n",
    "                   fpa_params=None,\n",
    "                   debug=False,\n",
    "                   modeling_params = None,\n",
    "                   **kwargs):\n",
    "    '''\n",
    "    This function did the basic data processing;\n",
    "    timestamp_column: the name of the time col in df;\n",
    "    upset_flag_config-debug: config data obtainted from **kwargs;\n",
    "    return:\n",
    "    \n",
    "    '''    \n",
    "    # convert string dates to real dates\n",
    "    if (timestamp_column in df.columns) and isinstance(df[timestamp_column].iloc[0], str):\n",
    "        df[timestamp_column] = pd.to_datetime(df[timestamp_column], utc=True)\n",
    "\n",
    "    # set the index\n",
    "    if df.index.name != timestamp_column:\n",
    "        df = df.set_index(timestamp_column)\n",
    "\n",
    "    # get the flag_all / flag_filtered based on the upset config\n",
    "    df_s = score_upset_subevent(df, upset_flag_config, remove_post_processing=True, remove_persistence=True)\n",
    "    df = df_s.join(df, how='outer')\n",
    "    # need to understand pre-processing from the above\n",
    "\n",
    "    # handle the special functions\n",
    "    # assumes the input data contains all the required columns (we checked\n",
    "    # above)\n",
    "    # computes the new functions of the appropriate columns\n",
    "    # deletes any columns that were requred for the computation, but\n",
    "    # only if they weren't in the original tag_names input\n",
    "\n",
    "    tags_to_drop = []\n",
    "    for s in special_functions:\n",
    "        print(\"Adding column: {}\".format(s))\n",
    "        df[s] = spfunc.functions[s](df)\n",
    "    tags_to_drop = []\n",
    "    for s in special_functions:\n",
    "        tspec = spfunc.tag_map[s]\n",
    "        for t in tspec:\n",
    "            if t not in tags:\n",
    "                tags_to_drop.append(t)\n",
    "    df = df.drop(columns=tags_to_drop)\n",
    "\n",
    "    # hande conversions/recoding\n",
    "    if 'conversions' in kwargs:\n",
    "        if 'null_to_zero_tags' in kwargs['conversions']:\n",
    "            for t in kwargs['conversions']['null_to_zero_tags']:\n",
    "                if t in df.columns:\n",
    "                    if debug:\n",
    "                        print('Converting null to 0 for {}'.format(t))\n",
    "                    df[t] = df[t].fillna(0)  # df.loc[df[t].isnull(), t] = 0\n",
    "\n",
    "    if 'null_conversions' in kwargs:\n",
    "        for t, v in kwargs['null_conversions']:\n",
    "            if t in df.columns:\n",
    "                if debug:\n",
    "                    print('{}: converting null to {}'.format(t, v))\n",
    "                df[t] = df[t].fillna(v)\n",
    "\n",
    "\n",
    "    # handle categorical variables, for each we train an encoder and\n",
    "    # save it for use during deployment\n",
    "    if mode == 'train':\n",
    "        trained_params['categorical_transformers'] = dict()\n",
    "        for c in categorical_tags:\n",
    "            ## check the datatype\n",
    "            if not isinstance(X, pd.Series):\n",
    "                raise ValueError(\"Data passed to one-hot encoding function is not a pandas.Series\")\n",
    "\n",
    "            ## transform into numpy values\n",
    "            v = X.values\n",
    "            colname = X.name\n",
    "\n",
    "            ## train encoder\n",
    "            encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "            mask = ~np.isnan(v)\n",
    "            encoder.fit(v[mask].reshape(-1, 1))\n",
    "\n",
    "            ## transform and change dtype\n",
    "            v_enc = encoder.transform(v[mask].reshape(-1, 1))\n",
    "            v_enc_final = np.zeros(shape=(mask.shape[0], v_enc.shape[1]))\n",
    "            v_enc_final[mask] = v_enc\n",
    "            \n",
    "            ## rename and add new col\n",
    "            onehot_nf = \"{}_cat={}\"\n",
    "            names = [onehot_nf.format(colname, af) for af in encoder.categories_[0]]\n",
    "            new_cols = pd.DataFrame(v_enc_final, index=index, columns=names)\n",
    "            \n",
    "            # drop the column\n",
    "            df = df.drop(columns=[c])\n",
    "            \n",
    "            # add the new columns\n",
    "            df = pd.concat([df, new_cols], axis=1)\n",
    "    else:\n",
    "        raise ValueError('Unknown mode: {}'.format(mode))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find extra tags if any, named as tag_extended\n",
    "tags_extended = copy.copy(modeling_params['tags'])\n",
    "spfunc = SpecialFunctions()\n",
    "for s in modeling_params['special_functions']:\n",
    "    for t in spfunc.tag_map[s]:\n",
    "        if t not in tags_extended:\n",
    "            tags_extended.append(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpa_sensors_and_target_preparation(self,\n",
    "                                       df,\n",
    "                                       mode='test',\n",
    "                                       debug=False,\n",
    "                                       fpa_params=None,\n",
    "                                       timestamp_column='timestamp_utc',\n",
    "                                       **kwargs):\n",
    "\n",
    "    '''\n",
    "        This function...\n",
    "        df: data frame with indexed timestamp column with name given by timestamp_column;\n",
    "        fpa_params: pre-defined dict to \n",
    "        \n",
    "\n",
    "    '''\n",
    "    # need to handle both millennium and steepbank\n",
    "\n",
    "    flag_all = 'flag_all'\n",
    "    flag_filtered = 'flag_filtered'\n",
    "    flag_column = flag_all\n",
    "    \n",
    "    ## separate info in fpa_param and store in variable \n",
    "    if fpa_params is None:\n",
    "        raise ValueError('Must supply appropriate FPA parameters')\n",
    "        # Window size for rolling average during feature generation\n",
    "    else:\n",
    "        stages = fpa_params['stages']\n",
    "        # Future prediction window size\n",
    "        failure_detection_window = fpa_params['failure_detection_window']\n",
    "        dead_period = fpa_params['dead_period']\n",
    "\n",
    "    if debug:\n",
    "        print('Mode: {}'.format(mode))\n",
    "        print(df[flag_filtered].value_counts())\n",
    "        # print(df.target_num.describe())\n",
    "        print(df.columns.tolist())\n",
    "\n",
    "    ## 0. Input dataframe\n",
    "    # The created dataframe consists of all related senors (raw values) and variable to indicate upset events\n",
    "\n",
    "    sensor_table = df.copy().reset_index()\n",
    "\n",
    "    ### 1. Loading the Sensor Data\n",
    "\n",
    "    # Create the appropriate sensor data frame to agree with FPA flow. The sensor dataframe should only consist of sensor readings and timestamp and asset identifier columns.\n",
    "\n",
    "    # User needs to specify the asset_id column name is specified in the variable 'sensor_asset_id' and the date column name is specified in the variable 'sensor_date'. Finally, the format of the date needs to specified using [python's datetime format.](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)\n",
    "    # to do: don't forward fill the target_num_2\n",
    "\n",
    "    if mode == 'train':\n",
    "        if debug:\n",
    "            print('* Forward filling missing values *')\n",
    "        sensor_table = sensor_table.fillna(method='ffill')\n",
    "\n",
    "    sensor_table['asset_id'] = 1\n",
    "\n",
    "    # sensor_table.describe()\n",
    "    # sensor_table.head(10)\n",
    "\n",
    "    # input_manager = InputDataManager(\"object_storage\", params=dict(credentials=credentials, filename=\"sensor_data.csv\"))\n",
    "    # sensor_table = input_manager.get_pandas_data_frame()\n",
    "\n",
    "    sensor_asset_id = 'asset_id'\n",
    "    sensor_date = timestamp_column\n",
    "    sensor_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "    # Drop duplicates is problematic because of daylight savings time issue -- changed to UTC properly\n",
    "    # sensor_table = sensor_table.drop_duplicates(subset=[sensor_asset_id, sensor_date])\n",
    "    sensor_table[sensor_date] = pd.to_datetime(sensor_table[sensor_date], format=sensor_date_format)\n",
    "\n",
    "    # # # # #\n",
    "    # Create failure table and associated features\n",
    "    failure_table = df[df[flag_filtered] == 1].reset_index()[[timestamp_column, flag_all]].copy()\n",
    "    failure_table['asset_id'] = 1\n",
    "    # rename columns to agree with FPA framework -- we have only one failure ID (1)\n",
    "    failure_table.columns = [timestamp_column, 'failure_id', 'asset_id']\n",
    "\n",
    "    failure_asset_id = 'asset_id'\n",
    "    failure_date = timestamp_column\n",
    "    failure_id = 'failure_id'\n",
    "    failure_date_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "    failure_table[failure_date] = pd.to_datetime(failure_table[failure_date], format=failure_date_format)\n",
    "    # for suncor all failures are the same type\n",
    "    failure_table[failure_id] = 1\n",
    "\n",
    "    failure_table = failure_table.drop_duplicates(subset=[failure_asset_id, failure_date])\n",
    "    failure_table = failure_table.dropna()\n",
    "\n",
    "    '''\n",
    "    ### Define failure-specific features\n",
    "    Define some features directly related to frequency and time since last failure event\n",
    "    '''\n",
    "\n",
    "    failure_feature = df.reset_index()[[timestamp_column, flag_column]].copy()\n",
    "    failure_feature['asset_id'] = 1\n",
    "    failure_feature['active_failure'] = failure_feature[flag_column].shift(1)\n",
    "    failure_feature_names = ['active_failure']\n",
    "\n",
    "    # there is a problem with the binning of failure intervals\n",
    "    if False:\n",
    "        failure_feature = df.reset_index()[[timestamp_column, flag_column]].copy()\n",
    "        failure_feature['asset_id'] = 1\n",
    "        failure_feature.columns = [timestamp_column, flag_column, 'asset_id']\n",
    "        failure_feature[failure_date] = pd.to_datetime(failure_feature[failure_date], format=failure_date_format)\n",
    "        failure_feature['active_failure'] = failure_feature[flag_column].shift(1)\n",
    "\n",
    "        failure_feature['cumsum'] = failure_feature['active_failure'].cumsum()\n",
    "        failure_feature['tmp'] = 1 - failure_feature['active_failure']\n",
    "        failure_feature['time_since_failure'] = failure_feature.groupby(['cumsum'])['tmp'].cumsum()\n",
    "        # failure_feature = failure_feature.drop(columns=['tmp', 'cumsum', 'Flag_all'])\n",
    "\n",
    "        failure_interarrival = failure_feature.groupby(['cumsum'])['time_since_failure'].max()\n",
    "        failure_interarrival_raw = failure_interarrival[failure_interarrival > 0].values.reshape(-1, 1)\n",
    "\n",
    "        failure_feature_names = ['active_failure']\n",
    "\n",
    "        if mode=='train':\n",
    "            n_bins = 4\n",
    "            kbd = KBinsDiscretizer(n_bins=n_bins, strategy='quantile', encode='onehot-dense')\n",
    "            kbd.fit(failure_interarrival_raw)\n",
    "            self.trained_params['time_since_failure_bin_model'] = kbd\n",
    "        else:\n",
    "            # assume pre-trained kbd in trained_params\n",
    "            kbd = self.trained_params['time_since_failure_bin_model']\n",
    "\n",
    "        n_bins = kbd.n_bins\n",
    "\n",
    "        out = np.zeros(shape=(failure_feature.shape[0], n_bins))*np.NaN\n",
    "        out[~failure_feature['time_since_failure'].isna(), ] = kbd.transform(\n",
    "            failure_feature['time_since_failure'][~failure_feature['time_since_failure'].isna()].values.reshape(-1,1))\n",
    "\n",
    "        for b in range(n_bins):\n",
    "            label = \"time_since_failure__bin_{}\".format(b)\n",
    "            failure_feature[label] = out[:, b]\n",
    "            failure_feature_names.append(label)\n",
    "\n",
    "    # merge features to create overall sensor table\n",
    "    assert (sensor_table[timestamp_column] == failure_feature[timestamp_column]).all(), \"Mismatch in timestamp column\"\n",
    "    sensor_table = pd.concat([sensor_table, failure_feature[failure_feature_names]], axis=1)\n",
    "\n",
    "    # # # # #\n",
    "    # Final feature generation on the sensors tables\n",
    "    sensor_variables = sensor_table.columns.tolist()\n",
    "    sensor_variables.remove(timestamp_column)\n",
    "    sensor_variables.remove('asset_id')\n",
    "    sensor_variables.remove(flag_all)\n",
    "    sensor_variables.remove(flag_filtered)\n",
    "\n",
    "    # create list of all categorical columns, using expanded names\n",
    "    categorical_columns = ['active_failure']\n",
    "    for x in self.trained_params['categorical_transformers'].values():\n",
    "        categorical_columns.extend(x.colnames)\n",
    "\n",
    "    # remove categorical from list of all sensors\n",
    "    for c in categorical_columns:\n",
    "        sensor_variables.remove(c)\n",
    "\n",
    "    stage_sensor_table = sensor_table.copy()\n",
    "    sensor_date = 'datetime'\n",
    "    stage_sensor_table = stage_sensor_table.rename(columns={timestamp_column: 'datetime'})\n",
    "\n",
    "    # sensor to add at end -- these are only available in training mode\n",
    "    sensors_to_add = stage_sensor_table[[sensor_date, sensor_asset_id, flag_all, flag_filtered]].copy()\n",
    "\n",
    "    # split categorical and numeric dataframes\n",
    "    if len(categorical_columns) > 0:\n",
    "        stage_sensor_table_num = stage_sensor_table.drop(columns=categorical_columns)\n",
    "    else:\n",
    "        stage_sensor_table_num = stage_sensor_table.copy()\n",
    "\n",
    "    if len(sensor_variables) > 0:\n",
    "        stage_sensor_table_cat = stage_sensor_table.drop(columns=sensor_variables)\n",
    "    else:\n",
    "        stage_sensor_table_cat = stage_sensor_table.copy()\n",
    "    stage_variables_num = copy.copy(sensor_variables)\n",
    "    stage_variables_cat = copy.copy(categorical_columns)\n",
    "\n",
    "    if debug:\n",
    "        print('Numerical variables:')\n",
    "        print(stage_variables_num)\n",
    "\n",
    "        print('Categorical variables:')\n",
    "        print(stage_variables_cat)\n",
    "\n",
    "    for stage in stages:\n",
    "        features_num = []\n",
    "        features_cat = []\n",
    "        tags = stage['tags']\n",
    "        if tags not in ['all', 'numeric', 'categorical']:\n",
    "            raise ValueError('Invalid tags processing directive in FPA configuration')\n",
    "\n",
    "        for hws, agg_simple, agg_adv in zip(stage['window_sizes'],\n",
    "                                            stage['simple_functions'],\n",
    "                                            stage['advanced_functions']):\n",
    "\n",
    "            if debug:\n",
    "                print(\"Generating features: {}\".format(hws))\n",
    "                print(\"Simple aggregtions list: \", agg_simple)\n",
    "                print(\"Advanced aggregtions list: \", agg_adv)\n",
    "\n",
    "            if len(stage_variables_num) > 0 and tags in ['all', 'numeric']:\n",
    "                if debug:\n",
    "                    print(\"Processing {} numeric variables...\".format(len(stage_variables_num)))\n",
    "\n",
    "                features_num_t = process_stage_nosrom(\n",
    "                    agg_simple, agg_adv, stage_sensor_table_num, hws,\n",
    "                    stage_variables_num, sensor_asset_id, sensor_date,\n",
    "                    sensor_date_format, debug=debug)\n",
    "                features_num.extend(features_num_t)\n",
    "\n",
    "            if len(stage_variables_cat) > 0 and tags in ['all', 'categorical']:\n",
    "                if debug:\n",
    "                    print(\"Processing {} categorical variables...\".format(len(stage_variables_cat)))\n",
    "\n",
    "                features_cat_t = process_stage_nosrom(\n",
    "                    agg_simple, agg_adv, stage_sensor_table_cat, hws,\n",
    "                    stage_variables_cat, sensor_asset_id, sensor_date,\n",
    "                    sensor_date_format, debug=debug)\n",
    "                features_cat.extend(features_cat_t)\n",
    "\n",
    "        # numeric feature merge\n",
    "        if len(features_num) > 0:\n",
    "            stage_sensor_table_num = merge_features(features_num, stage_variables_num, sensor_asset_id, sensor_date)\n",
    "            stage_variables_num = stage_sensor_table_num.columns.tolist()\n",
    "            if debug:\n",
    "                print(stage_variables_num)\n",
    "            stage_variables_num.remove(sensor_asset_id)\n",
    "            stage_variables_num.remove(sensor_date)\n",
    "\n",
    "        # categorical feature merge\n",
    "        if len(features_cat) > 0:\n",
    "            stage_sensor_table_cat = merge_features(features_cat, stage_variables_cat, sensor_asset_id, sensor_date)\n",
    "            stage_variables_cat = stage_sensor_table_cat.columns.tolist()\n",
    "            if debug:\n",
    "                print(stage_variables_cat)\n",
    "            stage_variables_cat.remove(sensor_asset_id)\n",
    "            stage_variables_cat.remove(sensor_date)\n",
    "\n",
    "    # final merge\n",
    "    sensor_features_all = pd.merge(stage_sensor_table_num,\n",
    "                                   stage_sensor_table_cat,\n",
    "                                   on=[sensor_asset_id, sensor_date],\n",
    "                                   how='left')\n",
    "    sensor_features_all = pd.merge(sensor_features_all,\n",
    "                                   sensors_to_add,\n",
    "                                   on=[sensor_asset_id, sensor_date],\n",
    "                                   how='left')\n",
    "\n",
    "    if mode == 'train':\n",
    "        # # # # #\n",
    "        # Generate failure keys and failure targets\n",
    "        # only needed if we are training a new model\n",
    "\n",
    "        failure_keys = sensor_features_all[['asset_id', 'datetime']].copy()\n",
    "        failure_keys.shape\n",
    "\n",
    "        # Failure window starts before dead period, so failure window runs from:\n",
    "        # t=-(failure_detection_window+dead_period) to t=-dead_period\n",
    "        # assuming failure is at time t=0\n",
    "        #\n",
    "        # the target_label in the dead period is assigned a value of -1\n",
    "\n",
    "        failure_target_table = generate_failure_targets_new(failure_table,\n",
    "                                                        failure_keys,\n",
    "                                                        failure_detection_window,\n",
    "                                                        failure_asset_id,\n",
    "                                                        failure_date,\n",
    "                                                        failure_id,\n",
    "                                                        dead_period=dead_period)\n",
    "\n",
    "        # Note outer join, this is equivalent of a union in set operations\n",
    "        # Where there are no matching keys, you get NaN values\n",
    "        FPA_table = pd.merge(failure_target_table, sensor_features_all, on=['asset_id', 'datetime'], how='outer')\n",
    "        # let's look at some values where we have flagged a failure\n",
    "\n",
    "        final_columns = FPA_table.columns.tolist()\n",
    "        final_columns.remove('asset_id')\n",
    "        final_columns.remove('datetime')\n",
    "        final_columns.remove('target_label')\n",
    "        for x in sensors_to_add.columns:\n",
    "            if x in final_columns:\n",
    "                final_columns.remove(x)\n",
    "        self.trained_params['final_columns'] = final_columns\n",
    "        if debug:\n",
    "            print(FPA_table.head())\n",
    "            print(FPA_table.target_label.value_counts())\n",
    "            print(FPA_table.shape)\n",
    "\n",
    "        # need to also return column list, and any other parameters to save the model json\n",
    "        # no -- leave it to the notebook\n",
    "        return FPA_table\n",
    "    elif mode == 'deploy':\n",
    "        # in deploy mode we return a matrix and we further insure order of columns\n",
    "        final_columns = self.trained_params['final_columns']\n",
    "\n",
    "        missing_cols = (set(final_columns) - set(sensor_features_all.columns.tolist()))\n",
    "        if len(missing_cols) > 0:\n",
    "            raise RuntimeError('Columns produced in feature engineering do not match expected, missing: {}'.format(\" \".join(list(missing_cols))))\n",
    "\n",
    "        sensor_features_all = sensor_features_all.loc[:, final_columns]\n",
    "\n",
    "        idx_null = (sensor_features_all.isnull().sum(axis=1) > 0)\n",
    "\n",
    "        if sensor_features_all.iloc[-1,:].isnull().sum() > 0:\n",
    "            raise RuntimeError('Most recent row of output has at least one null value')\n",
    "        if idx_null.sum() > 0:\n",
    "            print('Dropping {} row(s) with at least one null value'.format(idx_null.sum()))\n",
    "            sensor_features_all.dropna(inplace=True)\n",
    "\n",
    "        return sensor_features_all\n",
    "    else:\n",
    "        raise ValueError('Unknown mode: {}'.format(mode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
